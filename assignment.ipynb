{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "215e382b-e90e-4bd7-98f2-bfe29917974c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "import matplotlib.pyplot as plt\n",
    "class DataPipeline:\n",
    "    def __init__(self,sales_data_path,**args):\n",
    "        self.sales_data_path=sales_data_path\n",
    "        self.user_details_api = args.get(\"user_details_api\", 'https://jsonplaceholder.typicode.com/users')\n",
    "        self.weather_api = args.get(\"weather_api\", 'https://api.weatherbit.io/v2.0/history/daily')\n",
    "        self.weather_api_key = args.get(\"weather_api_key\", '723ebf6e3dac4b9bad7f2f11e03bc367')\n",
    "        # default lat & lon pointing to walmart store\n",
    "        self.lat, self.lon = args.get(\"lat\", 36.358), args.get(\"lon\", -94.209)\n",
    "    def read_data(self):\n",
    "        self.sales_df=pd.read_csv(self.sales_data_path)\n",
    "        self.user_df=pd.read_json(self.user_details_api)[[\"id\",\"name\",\"username\",\"email\",\"address\"]]\n",
    "        weather_api_params=f'lat={self.lat}&lon={self.lon}&start_date={str(self.sales_df[\"order_date\"].min())}&end_date={self.sales_df[\"order_date\"].max()}'\n",
    "        weather_result=requests.get(f'{self.weather_api}?{weather_api_params}&key={self.weather_api_key}')\n",
    "        print(weather_result)\n",
    "        self.weather_df=pd.json_normalize(json.loads(weather_result)[\"data\"])[[\"pres\",\"slp\",\"wind_spd\",\"temp\",\"rh\",\"clouds\",\"precip\",\"snow\",\"datetime\"]]\n",
    "    def merge_to_single_df(self):\n",
    "        sales_user_df=self.sales_df.merge(self.user_df,how='left',left_on=[\"customer_id\"],right_on=[\"id\"]).drop(\"id\",axis=\"columns\")\n",
    "        return sales_user_df.merge(self.weather_df,how='left',left_on=\"order_date\",right_on=\"datetime\")\n",
    "    def make_db_connection(self,host,dbname,user,password):\n",
    "        self.conn=psycopg2.connect(host=host, dbname=dbname, user=user, password=password)\n",
    "    def write_to_db(self,df,table_name):\n",
    "        if type(df).__name__=='Series':\n",
    "            df=df.reset_index()\n",
    "        if 'index' in df.columns:\n",
    "            df=df.drop('index',axis='columns')\n",
    "        for i in df.columns:\n",
    "            if df[i].count()>0 and type(df[i][0]).__name__=='dict':\n",
    "                df[i]=df[i].apply(json.dumps)\n",
    "        tuples = [tuple(x) for x in df.to_numpy()]\n",
    "        cols = ','.join(list(df.columns))\n",
    "        query = \"INSERT INTO %s(%s) VALUES %%s\" % (table_name, cols)\n",
    "        cursor = self.conn.cursor()\n",
    "        try:\n",
    "            extras.execute_values(cursor, query, tuples)\n",
    "            self.conn.commit()\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            print(\"Error: %s\" % error)\n",
    "            self.conn.rollback()\n",
    "            cursor.close()\n",
    "            return 1\n",
    "        print(f\"{table_name} {df.shape[0]} records were inserted\")\n",
    "        cursor.close()\n",
    "class VisualizeData:\n",
    "    def __init__(self,dataframe):\n",
    "        self.df=dataframe\n",
    "    def show_histogram(self,x,y):\n",
    "        plt.hist(self.df[x], bins=20,bottom=5, weights=self.df[y], edgecolor='red')\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "        plt.title(f'Histogram for {x} and {y} Columns')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "efb0ce2f-8895-40da-8fec-e8463b0a08fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_data 1000 records were inserted\n",
      "product_sales_avg 50 records were inserted\n",
      "sales_per_cust 10 records were inserted\n",
      "monthly_sales 13 records were inserted\n",
      "quarterly_sales 5 records were inserted\n",
      "average_sales_rain_based 7 records were inserted\n",
      "average_sales_temp_based 11 records were inserted\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Intiliaze DataPipeline\n",
    "    pipe=DataPipeline(\"C:/Users/Mohamed/Downloads/sales_data.csv\")\n",
    "    #Read necessary data\n",
    "    pipe.read_data()\n",
    "    # merge all dataframes to single dataframe\n",
    "    df=pipe.merge_to_single_df()\n",
    "    \n",
    "    #Determine the average order quantity per product \n",
    "    product_avg=df.groupby('product_id',as_index=False)['quantity'].mean()\n",
    "    # Calculate total sales amount per customer\n",
    "    sales_per_cust=df.groupby('customer_id',as_index=False)['price'].sum()\n",
    "    sales_per_cust.rename(columns={'price': 'order_value'}, inplace=True)\n",
    "\n",
    "    df1=df.copy()\n",
    "    df1[\"order_date\"]=pd.to_datetime(df1[\"order_date\"])\n",
    "    df1.set_index('order_date',inplace=True)\n",
    "    \n",
    "    # Analyze sales trends over time (e.g., monthly or quarterly sales)\n",
    "    # Resample the data to get monthly or quarterly sales\n",
    "    # 'M' represents monthly, 'Q' represents quarterly\n",
    "    monthly_sales = df1['quantity'].resample('M').sum()\n",
    "    quarterly_sales = df1['quantity'].resample('Q').sum()\n",
    "    monthly_sales=monthly_sales.reset_index(name='monthly_sales_quantity')\n",
    "    quarterly_sales=quarterly_sales.reset_index(name='quarterly_sales_quantity')\n",
    "\n",
    "    temp_bins = 10\n",
    "    wind_speed_bins = 10\n",
    "    precip_bins = 10\n",
    "\n",
    "    # Categorize weather data into bins\n",
    "    # Make temperature and precip(rain) to ranges for analysis weather based sales\n",
    "    df1['temp_bin'] = pd.cut(df1['temp'], bins=temp_bins).astype(str)\n",
    "    df1['precip_bin'] = pd.cut(df1['precip'], bins=precip_bins).astype(str)\n",
    "\n",
    "    # Calculate the average sales amount per rain condition\n",
    "    average_sales_rain_based = df1.groupby(['precip_bin'])['quantity'].sum()\n",
    "    # Calculate the average sales amount per temperature condition\n",
    "    average_sales_temp_based = df1.groupby(['temp_bin'])['quantity'].sum()\n",
    "\n",
    "    average_sales_rain_based=average_sales_rain_based.reset_index(name='sale_count')\n",
    "    average_sales_temp_based=average_sales_temp_based.reset_index(name='sale_count')\n",
    "\n",
    "    # Database connection details\n",
    "    host = \"db.lzzqdwfecduvpsotjsim.supabase.co\"\n",
    "    dbname = \"postgres\"\n",
    "    user = \"postgres\"\n",
    "    password = \"#N!8_P?qyvsBg7.\"\n",
    "    \n",
    "    # call function to establish connection string\n",
    "    pipe.make_db_connection(host,dbname,user,password)\n",
    "    \n",
    "    #write data to table\n",
    "    pipe.write_to_db(df,'sales_data')\n",
    "    pipe.write_to_db(product_avg,'product_sales_avg')\n",
    "    pipe.write_to_db(sales_per_cust,'sales_per_cust')\n",
    "    pipe.write_to_db(monthly_sales,'monthly_sales')\n",
    "    pipe.write_to_db(quarterly_sales,'quarterly_sales')\n",
    "    pipe.write_to_db(average_sales_rain_based,'average_sales_rain_based')\n",
    "    pipe.write_to_db(average_sales_temp_based,'average_sales_temp_based')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
